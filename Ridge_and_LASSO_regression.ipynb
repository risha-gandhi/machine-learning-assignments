{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Copy of Assignment 3: Ridge-and-LASSO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/risha-gandhi/machine-learning-assignments/blob/main/Ridge_and_LASSO_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc_6Yf8fie3X"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsbjWKiiie3b"
      },
      "source": [
        "### Part 1: Load and prep the data (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im-hZ_Aqie3c"
      },
      "source": [
        "\n",
        "In this assignment we'll look at the affect of using regularization on linear regression models that we train. You will write code to train models that use different regularizers and different penalties and to analyze how this affects the model.\n",
        "\n",
        "For this assignment, we will only be using a very small subset of the data to do our analysis. This is not something you would usually do in practice, but is something we do for this assignment to simplify this complexity of this dataset. The data is pretty noisy and to get meaningful results to demonstrate the theoretical behavior, you would need to use a much more complicated set of features that would be a bit more tedious to work with.\n",
        "\n",
        "Use ``home_data.csv.``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxovDo9Kie3d",
        "outputId": "f5a58183-6a24-4033-b384-3cc8e2746b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "# TODO: Load the data using pandas - you can look at Assignment 1 if you have forgotten how to do this.\n",
        "sales = pd.read_csv(\"home_data.csv\")\n",
        "# Selects 1% of the data\n",
        "sales = sales.sample(frac=0.01, random_state=0) \n",
        "\n",
        "print(f'Number of points: {len(sales)}')\n",
        "sales.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points: 216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-18e528d3-ebc6-4cc4-b97c-2e2d2f6336f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17384</th>\n",
              "      <td>1453602313</td>\n",
              "      <td>20141029T000000</td>\n",
              "      <td>297000</td>\n",
              "      <td>2</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1430</td>\n",
              "      <td>1650</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1430</td>\n",
              "      <td>0</td>\n",
              "      <td>1999</td>\n",
              "      <td>0</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7222</td>\n",
              "      <td>-122.290</td>\n",
              "      <td>1430</td>\n",
              "      <td>1650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>722</th>\n",
              "      <td>2225059214</td>\n",
              "      <td>20140808T000000</td>\n",
              "      <td>1578000</td>\n",
              "      <td>4</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4670</td>\n",
              "      <td>51836</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>4670</td>\n",
              "      <td>0</td>\n",
              "      <td>1988</td>\n",
              "      <td>0</td>\n",
              "      <td>98005</td>\n",
              "      <td>47.6350</td>\n",
              "      <td>-122.164</td>\n",
              "      <td>4230</td>\n",
              "      <td>41075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2680</th>\n",
              "      <td>2768000270</td>\n",
              "      <td>20140625T000000</td>\n",
              "      <td>562100</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1440</td>\n",
              "      <td>3700</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1200</td>\n",
              "      <td>240</td>\n",
              "      <td>1914</td>\n",
              "      <td>0</td>\n",
              "      <td>98107</td>\n",
              "      <td>47.6707</td>\n",
              "      <td>-122.364</td>\n",
              "      <td>1440</td>\n",
              "      <td>4300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18754</th>\n",
              "      <td>6819100040</td>\n",
              "      <td>20140624T000000</td>\n",
              "      <td>631500</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1130</td>\n",
              "      <td>2640</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1130</td>\n",
              "      <td>0</td>\n",
              "      <td>1927</td>\n",
              "      <td>0</td>\n",
              "      <td>98109</td>\n",
              "      <td>47.6438</td>\n",
              "      <td>-122.357</td>\n",
              "      <td>1680</td>\n",
              "      <td>3200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14554</th>\n",
              "      <td>4027700666</td>\n",
              "      <td>20150426T000000</td>\n",
              "      <td>780000</td>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3180</td>\n",
              "      <td>9603</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>3180</td>\n",
              "      <td>0</td>\n",
              "      <td>2002</td>\n",
              "      <td>0</td>\n",
              "      <td>98155</td>\n",
              "      <td>47.7717</td>\n",
              "      <td>-122.277</td>\n",
              "      <td>2440</td>\n",
              "      <td>15261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18e528d3-ebc6-4cc4-b97c-2e2d2f6336f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18e528d3-ebc6-4cc4-b97c-2e2d2f6336f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18e528d3-ebc6-4cc4-b97c-2e2d2f6336f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               id             date    price  ...     long  sqft_living15  sqft_lot15\n",
              "17384  1453602313  20141029T000000   297000  ... -122.290           1430        1650\n",
              "722    2225059214  20140808T000000  1578000  ... -122.164           4230       41075\n",
              "2680   2768000270  20140625T000000   562100  ... -122.364           1440        4300\n",
              "18754  6819100040  20140624T000000   631500  ... -122.357           1680        3200\n",
              "14554  4027700666  20150426T000000   780000  ... -122.277           2440       15261\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz5zhMH1ie3h"
      },
      "source": [
        "First, we do a bit of feature engineering by creating features that represent the squares of each feature and the square root of each feature. One benefit of using regularization is you can include more features than necessary and you don't have to be as worried about overfitting since the model is regularized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3-mJSjVie3h",
        "outputId": "6c8865f8-8f8e-4e97-b907-aa151be0d09f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "# All of the features of interest\n",
        "features = [\n",
        "    'bedrooms', \n",
        "    'bathrooms',\n",
        "    'sqft_living', \n",
        "    'sqft_lot', \n",
        "    'floors', \n",
        "    'waterfront', \n",
        "    'view', \n",
        "    'condition', \n",
        "    'grade',\n",
        "    'sqft_above',\n",
        "    'sqft_basement',\n",
        "    'yr_built', \n",
        "    'yr_renovated'\n",
        "]\n",
        "\n",
        "# Compute the square and sqrt of each feature\n",
        "all_features = []\n",
        "for feat in features:\n",
        "    square_feat = feat + '_square' \n",
        "    sqrt_feat = feat + '_sqrt'\n",
        "    \n",
        "    sales[square_feat] = sales[feat] ** 2\n",
        "    sales[sqrt_feat] = sales[feat].apply(sqrt)\n",
        "    \n",
        "    all_features.extend([feat, square_feat, sqrt_feat])\n",
        "    \n",
        "print(sales.head())\n",
        "\n",
        "X = sales[all_features]\n",
        "y = sales['price']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               id             date  ...  yr_renovated_square  yr_renovated_sqrt\n",
            "17384  1453602313  20141029T000000  ...                    0                0.0\n",
            "722    2225059214  20140808T000000  ...                    0                0.0\n",
            "2680   2768000270  20140625T000000  ...                    0                0.0\n",
            "18754  6819100040  20140624T000000  ...                    0                0.0\n",
            "14554  4027700666  20150426T000000  ...                    0                0.0\n",
            "\n",
            "[5 rows x 47 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUjAXbHGie3k"
      },
      "source": [
        "Next, we will split the data set into training, validation, and test sets. For this assignment we will use 70% of the data to train, 10% for validation, and 20% to test. \n",
        "\n",
        "We have written most of the splitting for you, but we need you to figure out what the sizes should be in this case based off the numbers above. Remember, we use `random_state=6` to make sure the results are the same for everyone on this assignment. \n",
        "*Hint: You should print out the length of the datasets to make sure you got it right!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqF3MC6bie3k",
        "outputId": "d7aa3357-6ca1-4628-fb14-6ac4410fc45b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO Make train/test splits of the right size.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_and_validation, X_test, y_train_and_validation, y_test  =  train_test_split(X, y, test_size = 0.2, random_state = 6) # TODO: Make the test set 20%\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_and_validation, y_train_and_validation, test_size = 0.125, random_state = 6)# TODO: Make the val set 10% of the total data (not 10% of X_train_and_validation)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 39) (22, 39) (44, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jk7GvKhie3o"
      },
      "source": [
        "We first need to do a little bit more pre-processing to prepare the data for model training. Models like Ridge and LASSO assume the input features are standardized (mean 0, std. dev. 1) and the target values are centered (mean 0). If we do not do this, we might get some unpredictable results since we violate the assumption of the models!\n",
        "\n",
        "We can use Sklearn's Standard scaler to do this - for each feature we learn the mean $\\mu$ and standard deviation $\\sigma$ and then rescale that feature for each point. Ie., if that feature used to be $x$ it is now $\\frac{x - \\mu}{s}$.\n",
        "\n",
        "See [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more info. Note that we learn it on the training set and then apply it to the validation set and test set. We also center the depenent variable $y$-values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKJnzMt1ie3p"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "y_train = y_train - np.mean(y_train)\n",
        "\n",
        "X_test = scaler.transform(X_test)\n",
        "y_test = y_test - np.mean(y_train)\n",
        "\n",
        "X_val = scaler.transform(X_val)\n",
        "y_val = y_val - np.mean(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqtmxHfqie3s"
      },
      "source": [
        "### Part 2: Linear Regression (5 pts)\n",
        "As a baseline, we will first, train a regular `LinearRegression` model on the data using the features in `all_features` and compute its **test RMSE** . The RMSE is the square root of the mean squared error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D02psSd4ie3s",
        "outputId": "ad231765-aaa1-47c3-d016-f78711f0b899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO Train a linear regression model - you may need to import LinearRegression and mean_squared_error from sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# TODO - compute and compare the RMSE on the train and test data.\n",
        "train_rmse = math.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "test_rmse =  math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('Train', train_rmse)\n",
        "print('Test', test_rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 142457.31542346903\n",
            "Test 675086.2425484249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jyZ9qvvie3w"
      },
      "source": [
        "--- \n",
        "### Part 3: Ridge Regression (10pts)\n",
        "\n",
        "\n",
        "In this section, we will do some **hyper-parameter tuning** to find the optimal setting of the regularization constant $\\lambda$ for Ridge Regression. Remember that $\\lambda$ is the coefficient that controls how much the model is penalized for having large weights in the optimization function.\n",
        "\n",
        "$\\hat{w}_{ridge} = \\min_w RSS(w) + \\lambda \\left\\lVert w \\right\\rVert_2^2$\n",
        "\n",
        "where $\\left\\lVert w \\right\\rVert_2^2 = \\sum_{j=0}^D w_j^2$ is the L2 norm of the parameters. By default, `sklearn`'s `Ridge` class does not regularize the intercept - you should never regularize the intercept\n",
        "\n",
        "For this part of the assignment, you will be writing code to find the optimal setting of the penalty $\\lambda$. Below, we describe what steps you will want to have in your code to compute these values:\n",
        "\n",
        "*Implementation Details*\n",
        "* Use the following choices of L2 penalty: $[10^{-5}, 10^{-4}, ..., 10^4, 10^5]$. In Python, you can create a list of these numbers using `np.logspace(-5, 5, 11)`. \n",
        "* Use the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class from sklearn to train a Ridge Regression model on the **training** data. The **only** parameters you need to pass when constructing the Ridge model are `alpha`, which lets you specify what you want the L2 penalty to be, and `random_state=0` to avoid randomness.\n",
        "* Evaluate both the training error and the validation error for the model by reporting the RMSE of each dataset.\n",
        "* Put all of your results in a pandas `DataFrame` named `ridge_data` so you can analyze them later. The `ridge_data` should have a row for each L2 penalty you tried and should have the following columns:\n",
        "  * `l2_penalty`: The L2 penalty for that row\n",
        "  * `model`: The actual `Ridge` model object that was trained with that L2 penalty\n",
        "  * `train_rmse`: The training RMSE for that model\n",
        "  * `validation_rmse`: The validation RMSE for that model\n",
        "\n",
        "*Hints: Here is a  strategy that you might find helpful*\n",
        "* You will need a loop to loop over the possible L2 penalties. Try writing a lot of the code without a loop first if you're stuck to help you figure out how the pieces go together. You can safely ignore building up the result `DataFrame` at first, just print all the information out to start! \n",
        "* If you are running into troubles writing your loop, try to print values out to investigate what's going wrong.\n",
        "* Remember to use RMSE for calculating the error!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8J4dgYwie3w",
        "outputId": "6b222ae8-ba6a-4eea-e769-17d40801e2ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO \n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "l2_penalties = np.logspace(-5, 5, 11) # TODO: make the list of lambda values\n",
        "ridge_data = []\n",
        "\n",
        "for l2_penalty in l2_penalties:\n",
        "    print(l2_penalty)\n",
        "    model = Ridge(alpha= l2_penalty, random_state = 0)#TODO: Train a Ridge regression model using Ridge in sklearn. Remember that alpha is the lambda parameter from class. Use random_state=0\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    train_rmse = math.sqrt(mean_squared_error(y_train, y_pred)) #TODO: compute RMSE on the train set\n",
        "    validation_rmse = math.sqrt(mean_squared_error(y_val, y_pred_val)) # TODO: compute RMSE on the validation set\n",
        "    \n",
        "    # We maintain a list of dictionaries containing our results\n",
        "    ridge_data.append({\n",
        "        'l2_penalty': l2_penalty,\n",
        "        'model': model,\n",
        "        'train_rmse': train_rmse,\n",
        "        'validation_rmse': validation_rmse})\n",
        "    \n",
        "ridge_data = pd.DataFrame(ridge_data) # We will put this data into a datframe to make it easier to use later.\n",
        "print(ridge_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1e-05\n",
            "0.0001\n",
            "0.001\n",
            "0.01\n",
            "0.1\n",
            "1.0\n",
            "10.0\n",
            "100.0\n",
            "1000.0\n",
            "10000.0\n",
            "100000.0\n",
            "      l2_penalty  ... validation_rmse\n",
            "0        0.00001  ...   643241.226105\n",
            "1        0.00010  ...   643479.885343\n",
            "2        0.00100  ...   641921.798386\n",
            "3        0.01000  ...   614600.996092\n",
            "4        0.10000  ...   572434.297452\n",
            "5        1.00000  ...   550761.625114\n",
            "6       10.00000  ...   548518.261356\n",
            "7      100.00000  ...   567371.418909\n",
            "8     1000.00000  ...   641193.052000\n",
            "9    10000.00000  ...   762923.644311\n",
            "10  100000.00000  ...   797592.043545\n",
            "\n",
            "[11 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlepJn1Wie3z"
      },
      "source": [
        "As a sanity check, the cells below make sure you have a variable named `ridge_data` with the right number of rows and columns. If nothing is printed out, you pass this sanity check! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcWmH5ytie30"
      },
      "source": [
        "assert type(ridge_data) == pd.DataFrame\n",
        "assert len(ridge_data) == 11\n",
        "\n",
        "for col in ['l2_penalty', 'model', 'train_rmse', 'validation_rmse']:\n",
        "    assert col in ridge_data.columns, f'Missing column {col}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm3ruRpbie33"
      },
      "source": [
        "### Part 4: Investigating Ridge Regression Results (5pts)\n",
        "\n",
        "Next, let's investigate how the penalty affected the train and validation error by running the following plotting code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7N9wjsRie33",
        "outputId": "99f4e285-08db-4f34-93b0-22c4d6b44842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# Plot the validation RMSE as a blue line with dots\n",
        "plt.plot(ridge_data['l2_penalty'], ridge_data['validation_rmse'], \n",
        "         'b-o', label='Validation')\n",
        "# Plot the train RMSE as a red line dots\n",
        "plt.plot(ridge_data['l2_penalty'], ridge_data['train_rmse'], \n",
        "         'r-o', label='Train')\n",
        "\n",
        "# Make the x-axis log scale for readability\n",
        "plt.xscale('log')\n",
        "\n",
        "# Label the axes and make a legend\n",
        "plt.xlabel('l2_penalty')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8d8b8e1490>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm7O_q2qie36"
      },
      "source": [
        "Next, we want to actually look at which model we think will perform best. First we define a helper function that will be used to inspect the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHtM2zfie37"
      },
      "source": [
        "def print_coefficients(model, features):\n",
        "    \"\"\"\n",
        "    This function takes in a model column and a features column. \n",
        "    And prints the coefficient along with its feature name.\n",
        "    \"\"\"\n",
        "    feats = list(zip(model.coef_, features))\n",
        "    print(*feats, sep = \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYQrhnlnie3-"
      },
      "source": [
        "In the cell below, write code that uses the `ridge_data` `DataFrame` to select which L2 penalty we would choose based on the evaluations we did in the previous section. You should print out the following values to help you answer the next questions! \n",
        "* The best L2 penalty based on the model evaluations\n",
        "* Take the best model and evaluate its error on the **test** dataset. Report the number as an RMSE.\n",
        "* Call the `print_coefficients` function passing in the model itself and the features used so you can look at all of its coefficient values.\n",
        "\n",
        "To do this in `pandas`, you'll need to use the `idxmin()` function to find the index of the smallest value in a column and the `loc` property to access that index. As an example, suppose we had a `DataFrame` named `df`:\n",
        "\n",
        "| a | b | c |\n",
        "|---|---|---|\n",
        "| 1 | 2 | 3 |\n",
        "| 2 | 1 | 3 |\n",
        "| 3 | 2 | 1 |\n",
        "\n",
        "If we wrote the code \n",
        "```python\n",
        "index = df['b'].idxmin()\n",
        "row = df.loc[index]\n",
        "```\n",
        "\n",
        "It would first find the index of the smallest value in the `b` column and then uses the `.loc` property of the `DataFrame` to access that particular row. It will return a `Series` object (basically a Python dictionary) which means you can use syntax like `row['a']` to access a particular column of that row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXEkKGbie3_",
        "outputId": "64a4153e-fbfa-41ce-d202-49068229b9d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ridge_data.append({\n",
        "#         'l2_penalty': l2_penalty,\n",
        "#         'model': model,\n",
        "#         'train_rmse': train_rmse,\n",
        "#         'validation_rmse': validation_rmse})\n",
        "\n",
        "# TODO Print information about best L2 model\n",
        "index = ridge_data['validation_rmse'].idxmin()\n",
        "best_ridge = ridge_data.loc[index]#TODO: Which model has the lowest RMSE on the validation set?\n",
        "y_pred_test = best_ridge['model'].predict(X_test)\n",
        "\n",
        "test_rmse = math.sqrt(mean_squared_error(y_test, y_pred_test)) # TODO: compute the RMSE on the Test set.\n",
        "print('Best L2 Penalty', best_ridge['l2_penalty'])\n",
        "print('TEST RMSE', test_rmse)\n",
        "print_coefficients(best_ridge['model'], all_features) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best L2 Penalty 10.0\n",
            "TEST RMSE 626479.191939315\n",
            "(-12814.960695689748, 'bedrooms')\n",
            "(-5578.58562755392, 'bedrooms_square')\n",
            "(-19987.878641088646, 'bedrooms_sqrt')\n",
            "(-261.2331046005436, 'bathrooms')\n",
            "(139717.20930550515, 'bathrooms_square')\n",
            "(-50659.47327155908, 'bathrooms_sqrt')\n",
            "(23125.50416726878, 'sqft_living')\n",
            "(19304.65519120802, 'sqft_living_square')\n",
            "(14413.31922599656, 'sqft_living_sqrt')\n",
            "(18909.07641753274, 'sqft_lot')\n",
            "(21375.20111150273, 'sqft_lot_square')\n",
            "(-25079.695226768985, 'sqft_lot_sqrt')\n",
            "(-15962.664049881238, 'floors')\n",
            "(13961.951626360678, 'floors_square')\n",
            "(-27220.383772045923, 'floors_sqrt')\n",
            "(43800.14749775992, 'waterfront')\n",
            "(43800.147497760336, 'waterfront_square')\n",
            "(43800.14749776029, 'waterfront_sqrt')\n",
            "(-7513.023932352359, 'view')\n",
            "(11529.923304894017, 'view_square')\n",
            "(-12278.67285084605, 'view_sqrt')\n",
            "(4224.607744785577, 'condition')\n",
            "(4624.595688989772, 'condition_square')\n",
            "(4026.45544462523, 'condition_sqrt')\n",
            "(44871.77670857457, 'grade')\n",
            "(73624.90237512332, 'grade_square')\n",
            "(31314.956423923228, 'grade_sqrt')\n",
            "(22432.11950301662, 'sqft_above')\n",
            "(40851.914370094986, 'sqft_above_square')\n",
            "(1391.5234697641665, 'sqft_above_sqrt')\n",
            "(1331.3789329041192, 'sqft_basement')\n",
            "(-35037.04738328038, 'sqft_basement_square')\n",
            "(25425.774550790993, 'sqft_basement_sqrt')\n",
            "(-26030.276413735864, 'yr_built')\n",
            "(-25617.78684676185, 'yr_built_square')\n",
            "(-26232.134439889072, 'yr_built_sqrt')\n",
            "(4718.090111503858, 'yr_renovated')\n",
            "(5102.806925372955, 'yr_renovated_square')\n",
            "(4514.904467763998, 'yr_renovated_sqrt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Ic4imZie4B"
      },
      "source": [
        " **Question A:**  Based on your evaluations, which L2 penalty would you use?\n",
        "\n",
        "*I would choose a L2 penalty of 10*\n",
        "\n",
        " **Question B:**  For the model you chose for the Q2, what is its test RMSE?\n",
        "\n",
        "*The test RMSE is 626479.19*\n",
        "\n",
        " **Question C:**  For the model you chose in Q2, what is the number of features that have coefficient 0?\n",
        "\n",
        "*We have no features with coefficients of 0*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgVrfrxYie4C"
      },
      "source": [
        "--- \n",
        "### Part 5: LASSO Regression (5 pts)\n",
        "In this section you will do basically the exact same analysis you did with Ridge Regression, but using LASSO Regression instead. Remember that for LASSO we choose the parameters that minimize this quality metric instead \n",
        "\n",
        "$$\\hat{w}_{LASSO} = \\min_w RSS(w) + \\lambda \\left\\lVert w \\right\\rVert_1$$\n",
        "\n",
        "where $\\left\\lVert w \\right\\rVert_1 = \\sum_{j=0}^D | w_j |$ is the L1 norm of the parameter vector.\n",
        "\n",
        "We will use the same set of instructions for LASSO as we did for Ridge, except for the following differences. Please refer back to the Ridge Regression instructions and your code to see how these differences fit in!\n",
        "\n",
        "* Use the [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) model. Like before, the only parameters you need to pass in are `alpha` for the L1 penalty and `random_state=0`.\n",
        "* The range L1 penalties should be $[10, 10^2, ..., 10^7]$. In Python, this is `np.logspace(1, 7, num=7)`.\n",
        "* The result should be stored in a `DataFrame` named `lasso_data`. All the columns should have the same name and corresponding values except the penalty column should be called `l1_penalty`.\n",
        "* It is okay if your code prints some `ConvergenceWarning` warnings, these should not impact your results!.\n",
        "\n",
        "You do not need to worry about your code being redundant for this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNai1N-Bie4D",
        "outputId": "4bb94a7a-dafb-49ee-90a5-cf692281ce55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Compute a lasso regression. See the Ridge Regression section for a model of what to do here\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "l1_penalties = np.logspace(1, 7, num=7) # TODO: make the list of lambda values\n",
        "lasso_data = []\n",
        "\n",
        "for l1_penalty in l1_penalties:\n",
        "    print(l1_penalty)\n",
        "    model = Lasso(alpha= l1_penalty, random_state = 0)\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_val)\n",
        "\n",
        "    train_rmse = math.sqrt(mean_squared_error(y_train, y_pred)) # TODO: compute RMSE on the train set\n",
        "    validation_rmse = math.sqrt(mean_squared_error(y_val, y_pred_val)) # TODO: compute RMSE on the validation set\n",
        "    \n",
        "    # We maintain a list of dictionaries containing our results\n",
        "    lasso_data.append({\n",
        "        'l1_penalty': l1_penalty,\n",
        "        'model': model,\n",
        "        'train_rmse': train_rmse,\n",
        "        'validation_rmse': validation_rmse})\n",
        "    \n",
        "lasso_data = pd.DataFrame(lasso_data) # We will put this data into a datframe to make it easier to use later.\n",
        "print(lasso_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n",
            "100.0\n",
            "1000.0\n",
            "10000.0\n",
            "100000.0\n",
            "1000000.0\n",
            "10000000.0\n",
            "   l1_penalty  ... validation_rmse\n",
            "0        10.0  ...   574102.178744\n",
            "1       100.0  ...   561653.281862\n",
            "2      1000.0  ...   536906.548347\n",
            "3     10000.0  ...   555771.690167\n",
            "4    100000.0  ...   643989.904483\n",
            "5   1000000.0  ...   802207.512615\n",
            "6  10000000.0  ...   802207.512615\n",
            "\n",
            "[7 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e+12, tolerance: 1.913e+09\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.000e+12, tolerance: 1.913e+09\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+09, tolerance: 1.913e+09\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFAkG4Loie4G"
      },
      "source": [
        "Here is a sanity check like before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiQ7xWYUie4G"
      },
      "source": [
        "assert type(lasso_data) == pd.DataFrame\n",
        "assert len(lasso_data) == 7\n",
        "\n",
        "for col in ['l1_penalty', 'model', 'train_rmse', 'validation_rmse']:\n",
        "    assert col in lasso_data.columns, f'Missing column {col}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJu6d6QLie4J"
      },
      "source": [
        "### Part 6:  Investigating Lasso ( 5 pts)\n",
        "\n",
        "Like before, let's look at how the L1 penalty affects the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK2KHst0ie4J",
        "outputId": "62ef77f2-4b49-4639-be7d-98cd0d89875a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# Plot the validation RMSE as a blue line with dots\n",
        "\n",
        "plt.plot(lasso_data['l1_penalty'], lasso_data['validation_rmse'],\n",
        "         'b-o', label='Validation')\n",
        "\n",
        "# Plot the train RMSE as a red line dots\n",
        "plt.plot(lasso_data['l1_penalty'], lasso_data['train_rmse'],\n",
        "         'r-o', label='Train')\n",
        "\n",
        "# Make the x-axis log scale for readability\n",
        "plt.xscale('log')\n",
        "\n",
        "# Label the axes and make a legend\n",
        "plt.xlabel('l1_penalty')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8d8b25b390>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZD2YVJVie4N"
      },
      "source": [
        "Like before, in the cell below, write code that uses the `lasso_data` `DataFrame` to select which L1 penalty we would choose based on the evaluations we did in the previous section. You should print out the following values to help you answer the next questions! \n",
        "* The best L1 penalty based on the model evaluations\n",
        "* Take the best model and evaluate it on the test dataset and report its RMSE\n",
        "* Call the `print_coefficients` function passing in the model itself and the features used so you can look at all of its coefficient values. Note some of the values are `-0.0` which is the same as `0.0` for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNggOYdSie4O",
        "outputId": "0d8375d0-6e83-421d-dc99-b3886fab1e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO : What is the best lambda value to use? Print the values from above\n",
        "index = lasso_data['validation_rmse'].idxmin()\n",
        "\n",
        "model = best_lasso['model']\n",
        "model.fit(X_train, y_train)\n",
        "predict_test = model.predict(X_test)\n",
        "best_lasso = lasso_data.loc[index] # TODO\n",
        "test_rmse = math.sqrt(mean_squared_error(y_test, predict_test)) # TODO \n",
        "print('Best L1 Penalty', best_lasso['l1_penalty'])\n",
        "print('TEST RMSE', test_rmse)\n",
        "print_coefficients(best_lasso['model'], all_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best L1 Penalty 1000.0\n",
            "TEST RMSE 632259.577639702\n",
            "(-0.0, 'bedrooms')\n",
            "(-26062.072706583123, 'bedrooms_square')\n",
            "(-13168.48073855585, 'bedrooms_sqrt')\n",
            "(-26032.24899731306, 'bathrooms')\n",
            "(234001.09464515498, 'bathrooms_square')\n",
            "(-128285.03553762606, 'bathrooms_sqrt')\n",
            "(0.0, 'sqft_living')\n",
            "(-0.0, 'sqft_living_square')\n",
            "(99795.65838300061, 'sqft_living_sqrt')\n",
            "(71087.39322963884, 'sqft_lot')\n",
            "(0.0, 'sqft_lot_square')\n",
            "(-59249.925115629594, 'sqft_lot_sqrt')\n",
            "(-0.0, 'floors')\n",
            "(19590.52966922941, 'floors_square')\n",
            "(-49684.08521691517, 'floors_sqrt')\n",
            "(135069.2054741631, 'waterfront')\n",
            "(0.0, 'waterfront_square')\n",
            "(0.0, 'waterfront_sqrt')\n",
            "(-0.0, 'view')\n",
            "(474.070469125082, 'view_square')\n",
            "(-9085.05047578395, 'view_sqrt')\n",
            "(11852.073361063814, 'condition')\n",
            "(0.0, 'condition_square')\n",
            "(5550.624625552447, 'condition_sqrt')\n",
            "(0.0, 'grade')\n",
            "(147571.67381741997, 'grade_square')\n",
            "(0.0, 'grade_sqrt')\n",
            "(0.0, 'sqft_above')\n",
            "(32841.90056200978, 'sqft_above_square')\n",
            "(0.0, 'sqft_above_sqrt')\n",
            "(0.0, 'sqft_basement')\n",
            "(-47520.696968686, 'sqft_basement_square')\n",
            "(24793.415922608656, 'sqft_basement_sqrt')\n",
            "(-57115.24288430111, 'yr_built')\n",
            "(-0.0, 'yr_built_square')\n",
            "(-8892.479045201224, 'yr_built_sqrt')\n",
            "(3897.5539237661224, 'yr_renovated')\n",
            "(18120.38271493947, 'yr_renovated_square')\n",
            "(0.0, 'yr_renovated_sqrt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+09, tolerance: 1.913e+09\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJdbBh1hie4Q"
      },
      "source": [
        "**Question E:** Based on your evaluations, which L1 penalty would you use?\n",
        "*Best L1 penalty was 1000*\n",
        "\n",
        "**Question F:** For the model you chose, what is the test RMSE?\n",
        "*The test RMSE is 626479.19*\n",
        "\n",
        "**Question G:** For the model you chose, what Features did it choose to keep? I.e. for what features was the coefficient not 0?\n",
        "\n",
        "* The following coefficients were not 0: bedrooms_square, bedrooms_sqrt, bathrooms_square, bathrooms_sqrt, sqft_living_sqrt, sqft_lot, sqft_lot_sqrt, floors_square, floors_sqrt, waterfront, view_square, view_sqrt, condition, condition_sqrt, grade_square, sqft_above_square, sqft_basement_square, sqft_basement_sqrt, yr_built, yr_built_sqrt, yr_renovated, yr_renovated_square\n",
        "\n",
        "**Question H:**: Based on our experiments, which model do we expect to perform best in the future? LinearRegression? Ridge? Lasso? None of the above?\n",
        "* Lasso*"
      ]
    }
  ]
}